{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xABx3A-soZMM"
   },
   "source": [
    "#  CV3DST  ReID\n",
    "- to train a small ReID dataset with cross-entropy and triplet-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFYSSMiwpxSq"
   },
   "source": [
    "#### Install and import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15351,
     "status": "ok",
     "timestamp": 1658910410246,
     "user": {
      "displayName": "Максим Голдин",
      "userId": "08631407013133915158"
     },
     "user_tz": -180
    },
    "id": "KRMsynpFU6gh",
    "outputId": "cb3ecbe0-7e78-49d3-9aae-03fa8028006a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "reid_root_dir = \"..\"\n",
    "root_dir = '..'\n",
    "sys.path.append(os.path.join(reid_root_dir, 'src'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5345,
     "status": "ok",
     "timestamp": 1658910415580,
     "user": {
      "displayName": "Максим Голдин",
      "userId": "08631407013133915158"
     },
     "user_tz": -180
    },
    "id": "RGOohkAgo-hW",
    "outputId": "0a2d2db4-fe9d-4ec4-c430-822c006681ff"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tracker.data_track import MOT16Sequences\n",
    "from tracker.data_obj_detect import MOT16ObjDetect\n",
    "from tracker.object_detector import FRCNN_FPN\n",
    "from tracker.tracker import Tracker, ReIDTracker\n",
    "from tracker.utils import (plot_sequence, evaluate_mot_accums, get_mot_accum,\n",
    "                           evaluate_obj_detect, obj_detect_transforms)\n",
    "# Load helper code\n",
    "from market.datamanager import ImageDataManager\n",
    "from market.models import build_model\n",
    "from market import utils\n",
    "from market import metrics\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "import motmetrics as mm\n",
    "mm.lap.default_solver = 'lap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1658910682779,
     "user": {
      "displayName": "Максим Голдин",
      "userId": "08631407013133915158"
     },
     "user_tz": -180
    },
    "id": "eQTAqnFptwul"
   },
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "seq_name = 'MOT16-reid'  # We recommend to use this subset.\n",
    "data_dir = os.path.join(root_dir, 'data/MOT16')\n",
    "output_dir = os.path.join(root_dir, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5KgKxhmMm1r"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jV2c5yengHyC"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM5linhlJpC0"
   },
   "source": [
    "# Training a ReID Network\n",
    "\n",
    "train a simple ReID network on the Market data. we will use a ResNet34/ResNet50 neural network that extracts features from an input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxrvjE5dK-jU"
   },
   "source": [
    "Next, create the the DataManager for the Market dataset that will provide the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "filn9BvUKfYN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train transforms ...\n",
      "+ resize to 256x128\n",
      "+ random flip\n",
      "+ random crop (enlarge to 288x144 and crop 256x128)\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "Building test transforms ...\n",
      "+ resize to 256x128\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "=> Loading train (source) dataset\n",
      "=> Loaded Market1501\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |   751 |    12936 |         6\n",
      "  query    |   750 |     3368 |         6\n",
      "  gallery  |   751 |    15913 |         6\n",
      "  ----------------------------------------\n",
      "=> Loading test (target) dataset\n",
      "=> Loaded Market1501\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |   751 |    12936 |         6\n",
      "  query    |   750 |     3368 |         6\n",
      "  gallery  |   751 |    15913 |         6\n",
      "  ----------------------------------------\n",
      "\n",
      "\n",
      "  **************** Summary ****************\n",
      "  source            : ['market1501']\n",
      "  # source datasets : 1\n",
      "  # source ids      : 751\n",
      "  # source images   : 12936\n",
      "  # source cameras  : 6\n",
      "  target            : ['market1501']\n",
      "  *****************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datamanager = ImageDataManager(root=reid_root_dir, height=256,width=128, batch_size_train=32, \n",
    "                               workers=2, transforms=['random_flip', 'random_crop'])\n",
    "train_loader = datamanager.train_loader\n",
    "test_loader = datamanager.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2jjUmXPP__Y"
   },
   "source": [
    "Now, let's create a resnet34 model and move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lw5hhJ89KfVc"
   },
   "outputs": [],
   "source": [
    "model = build_model('resnet34', datamanager.num_train_pids, loss='softmax', pretrained=True)\n",
    "model = model.cuda()\n",
    "\n",
    "trainable_params = model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYKGh1BvQUf7"
   },
   "source": [
    "For training the network, we now need to choose an optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UJAHcBToKfTY"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(trainable_params, lr=0.0003, \n",
    "                             weight_decay=5e-4, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8_QAhVZQfXB"
   },
   "source": [
    "The network will be trained on a cross-entropy loss, i.e., the network needs to classify each image to it's identity class. For $n$ different people, we will have $n$ different classes.\n",
    "\n",
    "During evaluation, we ignore the last classification layer and work on the extracted $feat$-dimensional features. This feature vector should be very similar for the same instance, and not similar for different instances.\n",
    "\n",
    "In the following, you have to implement two distance measurements:\n",
    "- Euclidian squared distance.\n",
    "- Cosine similarity.\n",
    "\n",
    "You are not allowed to change the interface of the function. Please have a look at the [Pytorch documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XH3PcL2MYT9H"
   },
   "outputs": [],
   "source": [
    "\n",
    "def euclidean_squared_distance(input1, input2):\n",
    "    \"\"\"Computes euclidean squared distance.\n",
    "    Args:\n",
    "        input1 (torch.Tensor): 2-D feature matrix.\n",
    "        input2 (torch.Tensor): 2-D feature matrix.\n",
    "    Returns:\n",
    "        torch.Tensor: distance matrix.\n",
    "    \"\"\"\n",
    "    distmat = torch.cdist(input1, input2, p=2.0) \n",
    "    return distmat**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "n1231r74YUyF"
   },
   "outputs": [],
   "source": [
    "def cosine_distance(input1, input2):\n",
    "    \"\"\"Computes cosine distance.\n",
    "    Args:\n",
    "        input1 (torch.Tensor): 2-D feature matrix (m x feat).\n",
    "        input2 (torch.Tensor): 2-D feature matrix (n x feat).\n",
    "    Returns:\n",
    "        torch.Tensor: distance matrix (m x n).\n",
    "    \"\"\"\n",
    "\n",
    "    # Given that cos_sim(u, v) = dot(u, v) / (norm(u) * norm(v))\n",
    "    #                          = dot(u / norm(u), v / norm(v))\n",
    "    # We fist normalize the rows, before computing their dot products via transposition:\n",
    "    norm1 = input1.norm(dim=1)[:, None]\n",
    "    norm2 = input2.norm(dim=1)[:, None]\n",
    "    input1_norm = input1/norm1\n",
    "    input2_norm = input2/norm2\n",
    "    cosine_similarity = torch.mm(input1_norm, input2_norm.t())\n",
    "    distmat = 1 - cosine_similarity\n",
    "    return distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsCwff40aCtK"
   },
   "source": [
    "With the implemented distance measure, we can now implement the evaluation function. We extract features for the query set and for the gallery set and then build a distance matrix based on your implemented distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bBP5mxLYXTAi"
   },
   "outputs": [],
   "source": [
    "metric_fn = cosine_distance  # cosine_distance or euclidean_squared_distance\n",
    "def evaluate(model, test_loader, ranks=[1, 5, 10, 20]):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        print('Extracting features from query set...')\n",
    "        q_feat, q_pids, q_camids = utils.extract_features(model, test_loader['query'])\n",
    "        print('Done, obtained {}-by-{} matrix'.format(q_feat.size(0), q_feat.size(1)))\n",
    "\n",
    "        print('Extracting features from gallery set ...')\n",
    "        g_feat, g_pids, g_camids = utils.extract_features(model, test_loader['gallery'])\n",
    "        print('Done, obtained {}-by-{} matrix'.format(g_feat.size(0), g_feat.size(1)))\n",
    "        \n",
    "        distmat = metrics.compute_distance_matrix(q_feat, g_feat, metric_fn=metric_fn)\n",
    "        distmat = distmat.numpy()\n",
    "\n",
    "        print('Computing CMC and mAP ...')\n",
    "        cmc, mAP = metrics.eval_market1501(\n",
    "            distmat,\n",
    "            q_pids,\n",
    "            g_pids,\n",
    "            q_camids,\n",
    "            g_camids,\n",
    "            max_rank=50\n",
    "        )\n",
    "\n",
    "        print('** Results **')\n",
    "        print('mAP: {:.1%}'.format(mAP))\n",
    "        print('CMC curve')\n",
    "        for r in ranks:\n",
    "            print('Rank-{:<3}: {:.1%}'.format(r, cmc[r - 1]))\n",
    "        return cmc[0], mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gxtrj6HaqGU"
   },
   "source": [
    "Finally, we can implement the training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGZF58_zKfR2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 30\n",
    "EPOCH_EVAL_FREQ = 5\n",
    "PRINT_FREQ = 50\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    losses = utils.MetricMeter()\n",
    "    batch_time = utils.AverageMeter()\n",
    "    end = time.time()\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # Predict output.\n",
    "        imgs, pids = data['img'].cuda(), data['pid'].cuda()\n",
    "        output = model(imgs)\n",
    "        # Compute loss.\n",
    "        loss = criterion(output, pids)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        losses.update({'Loss': loss})\n",
    "        if (batch_idx + 1) % PRINT_FREQ == 0:\n",
    "            utils.print_statistics(batch_idx, num_batches, epoch, MAX_EPOCH, batch_time, losses)\n",
    "        end = time.time()\n",
    "        \n",
    "    if (epoch + 1) % EPOCH_EVAL_FREQ == 0 or epoch == MAX_EPOCH - 1:\n",
    "        rank1, mAP = evaluate(model, test_loader)\n",
    "        print('Epoch {0}/{1}: Rank1: {rank}, mAP: {map}'.format(\n",
    "                    epoch + 1, MAX_EPOCH, rank=rank1, map=mAP))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** Results **\n",
    "mAP: 56.6%\n",
    "CMC curve\n",
    "Rank-1  : 76.6%\n",
    "Rank-5  : 90.3%\n",
    "Rank-10 : 93.6%\n",
    "Rank-20 : 95.8%\n",
    "Epoch 30/30: Rank1: 0.7657363414764404, mAP: 0.565798018920044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WTIXl8e5zOL"
   },
   "source": [
    "# Part II - Triplet loss and hard negative mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V60_tz9l93xi"
   },
   "source": [
    "Now, we can combine both losses and train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardBatchMiningTripletLoss(torch.nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining of samples in a batch.\n",
    "    \n",
    "    Reference:\n",
    "        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "    Args:\n",
    "        margin (float, optional): margin for triplet. Default is 0.3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(HardBatchMiningTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = torch.nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).\n",
    "            targets (torch.LongTensor): ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        n = inputs.size(0)\n",
    "\n",
    "        # Compute the pairwise euclidean distance between all n feature vectors.\n",
    "\n",
    "        #distance_matrix = euclidean_squared_distance(inputs, inputs)\n",
    "        #distance_matrix = distance_matrix.clamp(min=1e-12).sqrt()\n",
    "        distance_matrix = torch.cdist(inputs, inputs, p=2.0) # clear euclidian dist \n",
    "        \n",
    "        # For each sample (image), find the hardest positive and hardest negative sample.\n",
    "        # The targets are a vector that encode the class label for each of the n samples.\n",
    "        # Pairs of samples with the SAME class can form a positive sample.\n",
    "        # Pairs of samples with a DIFFERENT class can form a negative sample.\n",
    "        #\n",
    "        # loop over all samples, and for each one\n",
    "        # find the hardest positive sample and the hardest negative sample.\n",
    "        # The distances are then added to the following lists.\n",
    "        # Positive pairs should be as close as possible, while \n",
    "        # negative pairs should be quite far apart. \n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        \n",
    "        distance_positive_pairs, distance_negative_pairs = [], []\n",
    "        for i in range(n):\n",
    "            row_dist = distance_matrix[i]\n",
    "            row_mask = mask[i]\n",
    "            hard_pos_dist = row_dist[row_mask].max().unsqueeze(0)\n",
    "            hard_neg_dist = row_dist[row_mask==0].min().unsqueeze(0)\n",
    "            distance_positive_pairs.append(hard_pos_dist)\n",
    "            distance_negative_pairs.append(hard_neg_dist) \n",
    "        distance_positive_pairs = torch.cat(distance_positive_pairs)\n",
    "        distance_negative_pairs = torch.cat(distance_negative_pairs)\n",
    "\n",
    "        # The ranking loss will compute the triplet loss with the margin.\n",
    "        # loss = max(0, -1*(neg_dist - pos_dist) + margin)\n",
    "        y = torch.ones_like(distance_negative_pairs)\n",
    "        return self.ranking_loss(distance_negative_pairs, distance_positive_pairs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "l9bam06cQb7q"
   },
   "outputs": [],
   "source": [
    "model = build_model('resnet34', datamanager.num_train_pids, loss='triplet', pretrained=True)\n",
    "model = model.cuda()\n",
    "\n",
    "trainable_params = model.parameters()\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=0.0003, \n",
    "                             weight_decay=5e-4, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mdJLCUKt5qJX"
   },
   "outputs": [],
   "source": [
    "class CombinedLoss(object):\n",
    "    def __init__(self, margin=0.3, weight_triplet=1.0, weight_ce=1.0):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.triplet_loss = HardBatchMiningTripletLoss() \n",
    "        self.cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "        self.weight_triplet = weight_triplet\n",
    "        self.weight_ce = weight_ce\n",
    "\n",
    "    def __call__(self, logits, features, gt_pids):\n",
    "        loss = 0.0\n",
    "        loss_summary = {}\n",
    "        if self.weight_triplet > 0.0:\n",
    "            loss_t = self.triplet_loss(features, gt_pids) * self.weight_triplet\n",
    "            loss += loss_t\n",
    "            loss_summary['Triplet Loss'] = loss_t\n",
    "\n",
    "        if self.weight_ce > 0.0:\n",
    "            loss_ce = self.cross_entropy(logits, gt_pids) * self.weight_ce\n",
    "            loss += loss_ce\n",
    "            loss_summary['CE Loss'] = loss_ce\n",
    "\n",
    "        loss_summary['Loss'] = loss\n",
    "        return loss, loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxjVrejKQb4y"
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 30\n",
    "EPOCH_EVAL_FREQ = 5\n",
    "PRINT_FREQ = 10\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "criterion = CombinedLoss(0.3, 1.0, 1.0) \n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    losses = utils.MetricMeter()\n",
    "    batch_time = utils.AverageMeter()\n",
    "    end = time.time()\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # Predict output.\n",
    "        imgs, pids = data['img'].cuda(), data['pid'].cuda()\n",
    "        logits, features = model(imgs)\n",
    "        # Compute loss.\n",
    "        loss, loss_summary = criterion(logits, features, pids)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        losses.update(loss_summary)\n",
    "        if (batch_idx + 1) % PRINT_FREQ == 0:\n",
    "            utils.print_statistics(batch_idx, num_batches, epoch, MAX_EPOCH, batch_time, losses)\n",
    "        end = time.time()\n",
    "        \n",
    "    if (epoch + 1) % EPOCH_EVAL_FREQ == 0 or epoch == MAX_EPOCH - 1:\n",
    "        rank1, mAP = evaluate(model, test_loader)\n",
    "        print('Epoch {0}/{1}: Rank1: {rank}, mAP: {map}'.format(\n",
    "                    epoch + 1, MAX_EPOCH, rank=rank1, map=mAP))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** Results **\n",
    "mAP: 55.5%\n",
    "CMC curve\n",
    "Rank-1  : 76.7%\n",
    "Rank-5  : 90.3%\n",
    "Rank-10 : 93.3%\n",
    "Rank-20 : 95.6%\n",
    "Epoch 30/30: Rank1: 0.7666270732879639, mAP: 0.5549070795028802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nSs-bUZqerq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Exercise_ReID.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:tracking]",
   "language": "python",
   "name": "conda-env-tracking-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
